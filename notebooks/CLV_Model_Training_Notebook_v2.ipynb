{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cdb89f",
   "metadata": {},
   "source": [
    "Supporting material:\n",
    "This article explains which encoding method should be used for categorical values and why\n",
    "https://samriddhl.medium.com/binary-encoding-vs-one-hot-encoding-choosing-the-right-approach-for-your-ml-models-9be4533e8772 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833085d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# define the target month and year\n",
    "target_months = [\"2025-01\", \"2025-02\", \"2025-03\"]\n",
    "cutoff_date = pd.to_datetime(\"2025-01-01 15:41:51+01:00\", format=\"ISO8601\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed8d836",
   "metadata": {},
   "source": [
    "## Step 1: Remove Useless columns\n",
    "In this step, we load all **Shopify** order files and combine them into a single DataFrame. We then select only valuable columns for our models.\n",
    "Columns we removed are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ec3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "RAW_DATA_PATH = os.path.join(\"..\", \"resources\", \"data\", \"raw\", \"clv\")\n",
    "PROCESSED_DATA_PATH = os.path.join(\"..\", \"resources\", \"data\", \"processed\", \"clv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"1_orders_selected_columns.csv\")\n",
    "\n",
    "# File paths\n",
    "order_files = [\n",
    "    os.path.join(RAW_DATA_PATH, \"orders_export_2.csv\"),\n",
    "    os.path.join(RAW_DATA_PATH, \"orders_export_3.csv\"),\n",
    "    os.path.join(RAW_DATA_PATH, \"orders_export_1.csv\"),\n",
    "]\n",
    "\n",
    "##########step 1 all orders in one dataframe##########\n",
    "# all orders tables in one dataframe\n",
    "raw_orders_df = pd.concat([pd.read_csv(file) for file in order_files], ignore_index=True)\n",
    "################step 2 Keep only usefull columns###############\n",
    "\n",
    "orders_cleaned_columns = raw_orders_df[[\"Name\", \"Email\", # identifiers\n",
    "                                        \"Total\", # order total\n",
    "                                        \"Discount Amount\", \n",
    "                                        \"Created at\", # order creation date\n",
    "                                        \"Lineitem quantity\",\n",
    "                                        \"Lineitem price\",\n",
    "                                        \"Refunded Amount\"]]\n",
    "\n",
    "# columns that we can add later are:\n",
    "# country,\n",
    "# Accepts Marketing\n",
    "# shipping city\n",
    "\n",
    "orders_cleaned_columns.to_csv(output_path, index=False)\n",
    "orders_cleaned_columns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e3140",
   "metadata": {},
   "source": [
    "## Step 2: Normalize strings, dates, numeric values and drop rows where the email is missing\n",
    "Columns with a missing email have an uknown customer, there is no value in predicting the clv for unknown customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d543fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"1_orders_selected_columns.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"2_orders_normalized_columns.csv\")\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Normalize columns\n",
    "df[\"Created at\"] = pd.to_datetime(df[\"Created at\"], format=\"ISO8601\")\n",
    "cols_to_float = [\"Total\", \"Discount Amount\", \"Lineitem quantity\", \"Lineitem price\", \"Refunded Amount\"]\n",
    "for col in cols_to_float:\n",
    "    raw_orders_df[col] = pd.to_numeric(raw_orders_df[col])\n",
    "\n",
    "df[\"Email\"] = df[\"Email\"].str.lower().str.strip()\n",
    "df = df[df[\"Email\"].notna() & (df[\"Email\"] != \"\")]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e168e1d",
   "metadata": {},
   "source": [
    "## Step 3: Combine Multiple Rows Into One Per Order\n",
    "Shopify splits a single order into multiple rows if it contains multiple unique items. This step identifies such rows using the 'Email' and 'Name' (which is the id of an order) fields, and merges them by combining their line items into a single list. Additionally, duplicate orders are filtered to keep only the fulfilled ones. The result is a dataset where each order occupies one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b339648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_file = os.path.join(PROCESSED_DATA_PATH, \"2_orders_normalized_columns.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"3_orders_have_one_row.csv\")\n",
    "\n",
    "orders_df = pd.read_csv(orders_file)\n",
    "\n",
    "# Create structured line item object per row including price\n",
    "orders_df[\"Lineitem_object\"] = orders_df.apply(\n",
    "    lambda row: {\n",
    "        \"qty\": int(row[\"Lineitem quantity\"]),\n",
    "        \"price\": float(row[\"Lineitem price\"])\n",
    "    },\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Group all line items per order\n",
    "lineitems_grouped = orders_df.groupby([\"Name\", \"Email\"])[\"Lineitem_object\"].agg(list).reset_index()\n",
    "\n",
    "# Keep only the first row per order for metadata\n",
    "first_rows = orders_df.drop_duplicates(subset=[\"Name\", \"Email\"], keep=\"first\")\n",
    "\n",
    "# Merge grouped line items back into first rows\n",
    "merged_orders = pd.merge(\n",
    "    first_rows.drop(columns=[\"Lineitem quantity\", \"Lineitem price\", \"Lineitem_object\"]),\n",
    "    lineitems_grouped,\n",
    "    on=[\"Name\", \"Email\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename column for clarity\n",
    "merged_orders.rename(columns={\"Lineitem_object\": \"Lineitems\"}, inplace=True)\n",
    "\n",
    "# Save the final one-row-per-order file\n",
    "merged_orders.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c724d60",
   "metadata": {},
   "source": [
    "## Step 4: Feature engineering for Line items\n",
    "for now the lineitems column per order looks like this:\\\n",
    "`[{'qty': 1, 'price': 29.95}, {'qty': 1, 'price': 34.95}, {'qty': 1, 'price': 29.95}, {'qty': 1, 'price': -4.74}]`\\\n",
    "We want to remove this column because it is unprocesseable for our ML models, it will be replaced by.\n",
    "- average item price \n",
    "- amount of items\n",
    "optionally we can later add the maximum and minimum item value\n",
    "We also notice that some items have a negative value, these are discount items. We will replace those with two columns:\n",
    "- discount_item_applied \n",
    "- discount_item_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7511a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"3_orders_have_one_row.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"4_orders_lineitems_summarized.csv\")\n",
    "\n",
    "def summarize_lineitems(items):\n",
    "    discounted_items = [item for item in items if item[\"price\"] < 0]\n",
    "    regular_items = [item for item in items if item[\"price\"] >= 0]\n",
    "\n",
    "    discount_item_applied = len(discounted_items) > 0\n",
    "    discount_item_value = sum(item[\"price\"] for item in discounted_items)\n",
    "\n",
    "    total_qty = sum(item[\"qty\"] for item in regular_items)\n",
    "    avg_item_price = (\n",
    "        sum(item[\"qty\"] * item[\"price\"] for item in regular_items) / total_qty\n",
    "        if total_qty > 0 else 0\n",
    "    )\n",
    "\n",
    "    return pd.Series({\n",
    "        \"total_qty\": total_qty,\n",
    "        \"avg_item_price\": avg_item_price,\n",
    "        \"discount_item_applied\": discount_item_applied,\n",
    "        \"discount_item_value\": discount_item_value\n",
    "    })\n",
    "\n",
    "# Apply to dataframe\n",
    "orders_df = pd.read_csv(input_path)\n",
    "orders_df[\"Lineitems\"] = orders_df[\"Lineitems\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "lineitem_summary = orders_df[\"Lineitems\"].apply(summarize_lineitems)\n",
    "orders_df = pd.concat([orders_df, lineitem_summary], axis=1)\n",
    "orders_df.drop(columns=[\"Lineitems\"], inplace=True)\n",
    "orders_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad177546",
   "metadata": {},
   "source": [
    "## Step 5 Orders: One row per customer\n",
    "This script transforms raw Shopify orders into a clean, customer-level dataset for modeling.  \n",
    "It extracts the year-month from each order date and groups by customer and month to compute:  \n",
    "- total spent, total discounts, refunded amounts, quantity, and average item price  \n",
    "- total number of orders and average order value per month  \n",
    "\n",
    "The data is pivoted to wide format (1 row per customer), with one column per metric per month.  \n",
    "It also computes lifetime metrics like \n",
    "- total orders \n",
    "- total items\n",
    "- average lifetime order value.  \n",
    "This structure is ideal for CLV modeling as it captures both **temporal trends** and **lifetime behavior**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d542686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"4_orders_lineitems_summarized.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"5_customer_monthly_lifetime_features.csv\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Ensure datetime format\n",
    "df[\"Created at\"] = pd.to_datetime(df[\"Created at\"], utc=True)\n",
    "\n",
    "# Define cutoff for Q1 2025 exclusion\n",
    "df_pre2025 = df[df[\"Created at\"] < cutoff_date]\n",
    "\n",
    "# Extract year-month for pivoting\n",
    "df[\"year_month\"] = df[\"Created at\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "# Aggregate monthly metrics\n",
    "monthly_agg = df.groupby([\"Email\", \"year_month\"]).agg(\n",
    "    total_spent=(\"Total\", \"sum\"),\n",
    "    total_discount_amount=(\"Discount Amount\", \"sum\"),\n",
    "    total_refunded_amount=(\"Refunded Amount\", \"sum\"),\n",
    "    total_item_quantity=(\"total_qty\", \"sum\"),\n",
    "    avg_item_price=(\"avg_item_price\", \"mean\"),\n",
    "    discount_item_applied=(\"discount_item_applied\", \"max\"),\n",
    "    discount_item_value=(\"discount_item_value\", \"sum\"),\n",
    "    total_amount_of_orders=(\"Name\", \"nunique\"),\n",
    "    avg_order_value=(\"Total\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "monthly_agg[\"average_item_qty_per_order\"] = (\n",
    "    monthly_agg[\"total_item_quantity\"] / monthly_agg[\"total_amount_of_orders\"]\n",
    ")\n",
    "\n",
    "# Pivot to wide format (Email as row index)\n",
    "monthly_pivot = monthly_agg.pivot(index=\"Email\", columns=\"year_month\")\n",
    "monthly_pivot.columns = [f\"{metric}_{month}\" for metric, month in monthly_pivot.columns]\n",
    "\n",
    "# Fix discount boolean columns\n",
    "discount_cols = [col for col in monthly_pivot.columns if \"discount_item_applied\" in col]\n",
    "monthly_pivot[discount_cols] = monthly_pivot[discount_cols].fillna(False).astype(bool)\n",
    "\n",
    "# Aggregate customer lifetime metrics\n",
    "lifetime_agg = df_pre2025.groupby(\"Email\").agg(\n",
    "    lifetime_amount_items=(\"total_qty\", \"sum\"),\n",
    "    lifetime_amount_orders=(\"Name\", \"nunique\"),\n",
    "    lifetime_avg_order_value=(\"Total\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Pre-2025 customer behavioral flags\n",
    "customer_flags = df_pre2025.groupby(\"Email\").agg(\n",
    "    first_order_date=(\"Created at\", \"min\"),\n",
    "    last_order_date=(\"Created at\", \"max\"),\n",
    "    pre2025_order_count=(\"Name\", \"nunique\")\n",
    ").reset_index()\n",
    "\n",
    "customer_flags[\"is_recurring_customer\"] = customer_flags[\"pre2025_order_count\"] > 1\n",
    "customer_flags[\"days_since_last_order\"] = (cutoff_date - customer_flags[\"last_order_date\"]).dt.days\n",
    "customer_flags[\"days_since_first_order\"] = (cutoff_date - customer_flags[\"first_order_date\"]).dt.days\n",
    "\n",
    "\n",
    "\n",
    "# Combine all features\n",
    "final_df = monthly_pivot.reset_index()\n",
    "final_df = final_df.merge(lifetime_agg, on=\"Email\", how=\"left\")\n",
    "final_df = final_df.merge(\n",
    "    customer_flags[[\"Email\", \"is_recurring_customer\", \"days_since_last_order\", \"days_since_first_order\"]],\n",
    "    on=\"Email\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill missing values due to no pre-2025 history\n",
    "final_df[\"is_recurring_customer\"] = final_df[\"is_recurring_customer\"].fillna(False).astype(bool)\n",
    "final_df[\"days_since_last_order\"] = final_df[\"days_since_last_order\"].fillna(-1).astype(int)\n",
    "final_df[\"days_since_first_order\"] = final_df[\"days_since_first_order\"].fillna(-1).astype(int)\n",
    "\n",
    "# Save result\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243a1a2",
   "metadata": {},
   "source": [
    "## Step 6 Klaviyo (optional): Add behavioural features from klaviyo\n",
    "This might not be needed depending on the model performance, some columns can be useful.\n",
    "- Email Marketing Consent\n",
    "- First Active\n",
    "- Last Active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_file = os.path.join(RAW_DATA_PATH, \"Klaviyo_everyone_email.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"1_klaviyo_selected_columns.csv\")\n",
    "# Read the Klaviyo customers file\n",
    "klaviyo_df = pd.read_csv(customers_file)\n",
    "# Keep only useful columns\n",
    "klaviyo_cleaned_columns = klaviyo_df[[\"Email\", \"Email Marketing Consent\", \n",
    "                                      \"First Active\", \"Last Active\",\n",
    "                                      \"Last Open\", \"Last Click\"]]\n",
    "# Normalize the Email column\n",
    "klaviyo_cleaned_columns[\"Email\"] = klaviyo_cleaned_columns[\"Email\"].str.lower().str.strip()\n",
    "klaviyo_cleaned_columns.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06144942",
   "metadata": {},
   "source": [
    "## Step 7 Klaviyo (optional)\n",
    "We creat numerical features of the behavioural features we have:\n",
    "\n",
    "**First active becomes** -> customer age or days since first active\n",
    "\n",
    "**Last active becomes** -> customer recency or days since last active\n",
    "\n",
    "**Email Marketing Consent** can have three values SUBSCRIBED - UNSUBSCRIBED - NEVER_SUBSCRIBED -> We turn those into three binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59441523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_file = os.path.join(PROCESSED_DATA_PATH, \"1_klaviyo_selected_columns.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"2_klaviyo_cleaned_columns.csv\")\n",
    "\n",
    "# Load file\n",
    "klaviyo_df = pd.read_csv(input_file)\n",
    "\n",
    "# Reference date: the day your pipeline runs\n",
    "reference_date = pd.to_datetime(datetime.today())\n",
    "\n",
    "# Normalize email\n",
    "klaviyo_df[\"Email\"] = klaviyo_df[\"Email\"].str.lower().str.strip()\n",
    "\n",
    "# Parse dates safely\n",
    "klaviyo_df[\"First Active\"] = pd.to_datetime(klaviyo_df[\"First Active\"])\n",
    "klaviyo_df[\"Last Active\"] = pd.to_datetime(klaviyo_df[\"Last Active\"])\n",
    "klaviyo_df[\"Last Open\"] = pd.to_datetime(klaviyo_df[\"Last Open\"])\n",
    "klaviyo_df[\"Last Click\"] = pd.to_datetime(klaviyo_df[\"Last Click\"])\n",
    "\n",
    "# Feature engineering: time deltas\n",
    "klaviyo_df[\"_days_since_first_active\"] = (reference_date - klaviyo_df[\"First Active\"]).dt.days\n",
    "klaviyo_df[\"_days_since_last_active\"] = (reference_date - klaviyo_df[\"Last Active\"]).dt.days\n",
    "klaviyo_df[\"_days_since_last_open\"] = (reference_date - klaviyo_df[\"Last Open\"]).dt.days\n",
    "klaviyo_df[\"_days_since_last_click\"] = (reference_date - klaviyo_df[\"Last Click\"]).dt.days\n",
    "\n",
    "# Handle missing values\n",
    "klaviyo_df[\"_days_since_first_active\"].fillna(9999, inplace=True)\n",
    "klaviyo_df[\"_days_since_last_active\"].fillna(9999, inplace=True)\n",
    "klaviyo_df[\"_days_since_last_open\"].fillna(9999, inplace=True)\n",
    "klaviyo_df[\"_days_since_last_click\"].fillna(9999, inplace=True)\n",
    "\n",
    "# One-hot encode Email Marketing Consent\n",
    "klaviyo_df = pd.get_dummies(\n",
    "    klaviyo_df,\n",
    "    columns=[\"Email Marketing Consent\"],\n",
    "    prefix=\"email_consent\"\n",
    ")\n",
    "\n",
    "# Final columns to keep: email, engineered features, and encoded consent flags\n",
    "final_columns = [\"Email\", \"_days_since_first_active\", \"_days_since_last_active\", \"_days_since_last_open\", \"_days_since_last_click\"] + \\\n",
    "                [col for col in klaviyo_df.columns if col.startswith(\"email_consent_\")]\n",
    "\n",
    "klaviyo_cleaned = klaviyo_df[final_columns]\n",
    "\n",
    "# Save to CSV\n",
    "klaviyo_cleaned.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa442df",
   "metadata": {},
   "source": [
    "## Step 8: Klaviyo (optional) (only required if step 6 and 7 were done) Merge customer data with customer order trends data\n",
    "We merge the cleaned Shopify order data with customer profile data from Klaviyo using the 'Email' field. The result is a dataset that combines behavioral (order-based) and demographic (profile-based) information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3372ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_path_1 = os.path.join(PROCESSED_DATA_PATH, \"5_customer_monthly_lifetime_features.csv\")\n",
    "input_path_2 = os.path.join(PROCESSED_DATA_PATH, \"2_klaviyo_cleaned_columns.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"8_merged_orders_customers.csv\")\n",
    "orders_df = pd.read_csv(input_path_1)\n",
    "customers_behaviour_df = pd.read_csv(input_path_2)\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    orders_df,\n",
    "    customers_behaviour_df,\n",
    "    on=\"Email\",\n",
    "    how=\"left\"\n",
    ")\n",
    "merged_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091b4d4",
   "metadata": {},
   "source": [
    "## Step 9: Klaviyo (optional) fill missing values when a customer is not registered in klaviyo\n",
    "After merging klaviyo data, we discovered that **278** customers out of **18285** do not have records in klaviyo. The values for all columns coming from klaviyo for these customers are missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"8_merged_orders_customers.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"9_final_cleaned_dataset.csv\")\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "df[\"_days_since_first_active\"].fillna(9999, inplace=True)\n",
    "df[\"_days_since_last_active\"].fillna(9999, inplace=True)\n",
    "df[\"_days_since_last_open\"].fillna(9999, inplace=True)\n",
    "df[\"_days_since_last_click\"].fillna(9999, inplace=True)\n",
    "\n",
    "df[\"email_consent_NEVER_SUBSCRIBED\"].fillna(False, inplace=True)\n",
    "df[\"email_consent_SUBSCRIBED\"].fillna(False, inplace=True)\n",
    "df[\"email_consent_UNSUBSCRIBED\"].fillna(False, inplace=True)\n",
    "\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c7d38",
   "metadata": {},
   "source": [
    "## Step 10 remove all dataleakage prone columns and create target month column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f81956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"9_final_cleaned_dataset.csv\")\n",
    "output_path = os.path.join(PROCESSED_DATA_PATH, \"10_dataset_no_leakage_clv_target.csv\")\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Create CLV target label\n",
    "# -------------------------------\n",
    "df[\"CLV_Jan_to_Mar_2025\"] = df[[f\"total_spent_{m}\" for m in target_months if f\"total_spent_{m}\" in df.columns]].sum(axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Drop all features that contain any of the target months\n",
    "# -------------------------------\n",
    "df = df[[col for col in df.columns if not any(m in col for m in target_months)]]\n",
    "\n",
    "\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa5c00",
   "metadata": {},
   "source": [
    "## Step 12: remove highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"10_dataset_no_leakage_clv_target.csv\")\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Optional: store original shape\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "# Keep only numeric columns for correlation analysis\n",
    "numeric_df = df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "# Compute absolute correlation matrix\n",
    "corr_matrix = numeric_df.corr().abs()\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap=\"coolwarm\", annot=False, fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap (absolute)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# # Identify highly correlated pairs (above 0.95)\n",
    "# threshold = 0.95\n",
    "# high_corr_pairs = [\n",
    "#     (col1, col2)\n",
    "#     for col1 in corr_matrix.columns\n",
    "#     for col2 in corr_matrix.columns\n",
    "#     if col1 != col2 and corr_matrix.loc[col1, col2] > threshold\n",
    "# ]\n",
    "\n",
    "# # Drop one feature from each pair\n",
    "# to_drop = set()\n",
    "# for col1, col2 in high_corr_pairs:\n",
    "#     if col1 not in to_drop and col2 not in to_drop:\n",
    "#         # Drop col2 arbitrarily — could use variance or importance later\n",
    "#         to_drop.add(col2)\n",
    "\n",
    "# print(\"Highly correlated features to drop:\", to_drop)\n",
    "\n",
    "# # Drop them from the original DataFrame\n",
    "# df_cleaned = df.drop(columns=list(to_drop))\n",
    "\n",
    "# print(\"New shape after dropping:\", df_cleaned.shape)\n",
    "\n",
    "# df_cleaned.to_csv(input_path, index=False)\n",
    "# print(\"Saved cleaned dataset to:\", input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936332b",
   "metadata": {},
   "source": [
    "## Step 13: Train/Test Split\n",
    "\n",
    "To evaluate how well our model generalizes to unseen customers, we split our dataset into training and testing sets.\n",
    "\n",
    "We use an 80/20 split, a common practice in supervised learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_path = os.path.join(PROCESSED_DATA_PATH, \"10_dataset_no_leakage_clv_target.csv\")\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Convert all boolean columns to integers (0/1)\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# Define target column\n",
    "target = \"CLV_Jan_to_Mar_2025\"\n",
    "\n",
    "# Preserve Email column separately for traceability\n",
    "email_series = df[\"Email\"]\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=[target, \"Email\"])\n",
    "y = df[target]\n",
    "\n",
    "# Split while maintaining email tracking\n",
    "X_train, X_test, y_train, y_test, emails_train, emails_test = train_test_split(\n",
    "    X, y, email_series, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e094070",
   "metadata": {},
   "source": [
    "## Step 14: Baseline for CLV Prediction\n",
    "Zero Spending: Predict 0 for all customers.\n",
    "\n",
    "Global Average: Predict average of 3-month spend across all customers.\n",
    "\n",
    "Customer-Specific Monthly Average × 3: Predict Total Amount Spent / # active months × 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6184c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "\n",
    "# Create and train dummy model\n",
    "# dummy = DummyRegressor(strategy=\"median\")\n",
    "dummy = DummyRegressor(strategy=\"constant\", constant=0.00)\n",
    "# dummy = DummyRegressor(strategy=\"mean\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred_dummy)\n",
    "rmse = root_mean_squared_error(y_test, y_pred_dummy)\n",
    "r2 = r2_score(y_test, y_pred_dummy)\n",
    "\n",
    "print(\"Dummy Regressor (constant):\")\n",
    "print(f\"MAE  = {mae:.2f}\")\n",
    "print(f\"RMSE = {rmse:.2f}\")\n",
    "print(f\"R²   = {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a61dc7",
   "metadata": {},
   "source": [
    "## Step 15: Choose, Train, and Evaluate Model\n",
    "\n",
    "### Boosting\n",
    "\n",
    "We will use **Gradient boosting Regressor**, a powerful and efficient model for tabular data.\n",
    "\n",
    "#### Why Gradient boosting:\n",
    "- Performs well on structured data\n",
    "- Handles missing values internally\n",
    "- Provides feature importance insights\n",
    "- Scales well for large datasets\n",
    "\n",
    "We will train the model and evaluate its performance using:\n",
    "- RMSE (Root Mean Squared Error)\n",
    "- MAE (Mean Absolute Error)\n",
    "- R² Score (explained variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f319d",
   "metadata": {},
   "source": [
    "#### Find the best parameters for XGBoost and LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36f3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from xgboost import XGBRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "\n",
    "# param_grid = {\n",
    "#     \"max_depth\": [3, 4, 6],\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "#     \"n_estimators\": [100, 300],\n",
    "#     \"min_child_weight\": [1, 5],\n",
    "#     \"subsample\": [0.6, 0.8],\n",
    "#     \"colsample_bytree\": [0.6, 0.8]\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(LGBMRegressor(random_state=42), param_grid,\n",
    "#                     cv=3, scoring='r2', verbose=0)\n",
    "# grid.fit(X_train, y_train)\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c801e9c",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "y_train = y_train.fillna(0)\n",
    "y_test = y_test.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Define XGBoost with best parameters from grid search\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=100,       # number of trees\n",
    "    learning_rate=0.05,      # step size shrinkage\n",
    "    max_depth=6,            # how deep each tree can go\n",
    "    subsample=0.6,          # random row sampling\n",
    "    colsample_bytree=0.8,   # random column sampling per tree\n",
    "    random_state=42,\n",
    "    n_jobs=-1,               # use all cores\n",
    "    min_child_weight= 5,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae_xgboost = mean_absolute_error(y_test, y_pred)\n",
    "rmse_xgboost = root_mean_squared_error(y_test, y_pred)\n",
    "r2_xgboost = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "print(f\"MAE  = {mae_xgboost:.2f}\")\n",
    "print(f\"RMSE = {rmse_xgboost:.2f}\")\n",
    "print(f\"R²   = {r2_xgboost:.2f}\")\n",
    "\n",
    "\n",
    "# Define LGBMRegressor with best parameters from grid search\n",
    "lgbm = LGBMRegressor(\n",
    "    n_estimators=100,       # number of trees\n",
    "    learning_rate=0.05,      # step size shrinkage\n",
    "    max_depth=6,            # how deep each tree can go\n",
    "    subsample=0.6,          # random row sampling\n",
    "    colsample_bytree=0.8,   # random column sampling per tree\n",
    "    random_state=42,\n",
    "    n_jobs=-1,               # use all cores\n",
    "    min_child_weight= 1,\n",
    "    force_row_wise=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = lgbm.predict(X_test, raw_score=True)\n",
    "\n",
    "# Evaluate\n",
    "mae_lgbm = mean_absolute_error(y_test, y_pred)\n",
    "rmse_lgbm = root_mean_squared_error(y_test, y_pred)\n",
    "r2_lgbm = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"LGBMRegressor Results:\")\n",
    "print(f\"MAE  = {mae_lgbm:.2f}\")\n",
    "print(f\"RMSE = {rmse_lgbm:.2f}\")\n",
    "print(f\"R²   = {r2_lgbm:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14114b9d",
   "metadata": {},
   "source": [
    "XGboost got 49% and LGBM 51%\n",
    "#### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e863db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model performance metrics\n",
    "rmse = rmse_xgboost\n",
    "mae = mae_xgboost\n",
    "r2 = r2_xgboost\n",
    "### Scatter plot of actual vs predicted CLV\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.xlabel(\"Actual CLV (Jan–Mar 2025)\")\n",
    "plt.ylabel(\"Predicted CLV\")\n",
    "plt.title(\"Actual vs Predicted CLV\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Risidual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True)\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Prediction Error (Residuals)\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Feature importance plot\n",
    "importances = xgb.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Sort by importance\n",
    "sorted_idx = importances.argsort()[-20:]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_idx)), importances[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"Top 20 Most Important Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79baa8",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Let's do the same with Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf2780",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25129ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "# Train linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "# Predict on test set\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mae_linear_regression = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_linear_regression = root_mean_squared_error(y_test, y_pred_lr)\n",
    "r2_linear_regression = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"MAE  = {mae_linear_regression:.2f}\")\n",
    "print(f\"RMSE = {rmse_linear_regression:.2f}\")\n",
    "print(f\"R²   = {r2_linear_regression:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c162644",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e26700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot 1: Actual vs Predicted ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred_lr, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
    "plt.xlabel(\"Actual CLV (Jan–Mar 2025)\")\n",
    "plt.ylabel(\"Predicted CLV\")\n",
    "plt.title(\"Linear Regression: Actual vs Predicted CLV\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 2: Residuals Distribution ===\n",
    "residuals = y_test - y_pred_lr\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True)\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Prediction Error (Residuals)\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Plot 3: Feature Coefficients ===\n",
    "coefs = pd.Series(lr_model.coef_, index=X_train.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "coefs.head(20).plot(kind='barh', color=\"green\")\n",
    "plt.title(\"Top 20 Positive Coefficients\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
