{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4161f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02edb7",
   "metadata": {},
   "source": [
    "## Step 1: Remove Useless columns\n",
    "In this step, we will load all **Shopify** orders and then select only valuable columns for our models.\n",
    "belong to clean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86dd8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Email   Total  Discount Amount  \\\n",
      "0                lvaane@gmail.com   81.48            20.37   \n",
      "1     Annekedooijewaard@gmail.com   70.06             7.79   \n",
      "2            hankebertram@msn.com   80.68            20.17   \n",
      "3            hankebertram@msn.com     NaN              NaN   \n",
      "4               svelmbt@gmail.com   47.61             5.29   \n",
      "...                           ...     ...              ...   \n",
      "5825           r.hurk@hotmail.com  246.99            43.56   \n",
      "5826           r.hurk@hotmail.com     NaN              NaN   \n",
      "5827           r.hurk@hotmail.com     NaN              NaN   \n",
      "5828     charlotte.roke@gmail.com   23.35             2.60   \n",
      "5829            kristy@pousset.be   43.11             4.79   \n",
      "\n",
      "                     Created at  Lineitem quantity  Lineitem price  \n",
      "0     2025-05-31 06:53:36 +0200                  3           33.95  \n",
      "1     2025-05-31 06:44:57 +0200                  3           25.95  \n",
      "2     2025-05-30 23:59:48 +0200                  2           32.95  \n",
      "3     2025-05-30 23:59:48 +0200                  1           34.95  \n",
      "4     2025-05-30 23:00:06 +0200                  1           25.95  \n",
      "...                         ...                ...             ...  \n",
      "5825  2025-03-01 08:41:35 +0100                  3           31.95  \n",
      "5826  2025-03-01 08:41:35 +0100                  3           32.95  \n",
      "5827  2025-03-01 08:41:35 +0100                  3           31.95  \n",
      "5828  2025-03-01 07:42:09 +0100                  1           25.95  \n",
      "5829  2025-03-01 07:18:13 +0100                  2           23.95  \n",
      "\n",
      "[5830 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_orders_df = pd.read_csv('orders_export_1.csv')\n",
    "orders_selected_columns = raw_orders_df[[\"Email\", # identifiers\n",
    "                                        \"Total\", # order total\n",
    "                                        \"Discount Amount\", \n",
    "                                        \"Created at\", # order creation date\n",
    "                                        \"Lineitem quantity\",\n",
    "                                        \"Lineitem price\"]]\n",
    "orders_selected_columns.to_csv(\"step1.csv\")\n",
    "\n",
    "print(orders_selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe13eea",
   "metadata": {},
   "source": [
    "## Step 2: Normalize strings, dates, numeric values and drop rows where the email is missing\n",
    "Columns with a missing email have an uknown customer, there is no value in predicting the NPD for unknown customers\n",
    "belong to clean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fba32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Email   Total  Discount Amount Created at  \\\n",
      "0                lvaane@gmail.com   81.48            20.37 2025-05-31   \n",
      "1     annekedooijewaard@gmail.com   70.06             7.79 2025-05-31   \n",
      "2            hankebertram@msn.com   80.68            20.17 2025-05-30   \n",
      "3            hankebertram@msn.com     NaN              NaN 2025-05-30   \n",
      "4               svelmbt@gmail.com   47.61             5.29 2025-05-30   \n",
      "...                           ...     ...              ...        ...   \n",
      "5825           r.hurk@hotmail.com  246.99            43.56 2025-03-01   \n",
      "5826           r.hurk@hotmail.com     NaN              NaN 2025-03-01   \n",
      "5827           r.hurk@hotmail.com     NaN              NaN 2025-03-01   \n",
      "5828     charlotte.roke@gmail.com   23.35             2.60 2025-03-01   \n",
      "5829            kristy@pousset.be   43.11             4.79 2025-03-01   \n",
      "\n",
      "      Lineitem quantity  Lineitem price  \n",
      "0                     3           33.95  \n",
      "1                     3           25.95  \n",
      "2                     2           32.95  \n",
      "3                     1           34.95  \n",
      "4                     1           25.95  \n",
      "...                 ...             ...  \n",
      "5825                  3           31.95  \n",
      "5826                  3           32.95  \n",
      "5827                  3           31.95  \n",
      "5828                  1           25.95  \n",
      "5829                  2           23.95  \n",
      "\n",
      "[5830 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdtas\\AppData\\Local\\Temp\\ipykernel_24176\\2478249081.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders_selected_columns[\"Created at\"] = pd.to_datetime(orders_selected_columns[\"Created at\"],utc=True)\n",
      "C:\\Users\\mdtas\\AppData\\Local\\Temp\\ipykernel_24176\\2478249081.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders_selected_columns[\"Email\"] = orders_selected_columns[\"Email\"].str.lower().str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Normalize columns\n",
    "orders_selected_columns[\"Created at\"] = pd.to_datetime(orders_selected_columns[\"Created at\"],utc=True)\n",
    "\n",
    "# We have some columns \"Total\", \"Discount Amount\", and \"Lineitem quantity\" that may currently be strings.So convert them into float\n",
    "\n",
    "cols_to_float = [\"Total\", \"Discount Amount\", \"Lineitem quantity\"]\n",
    "for col in cols_to_float:\n",
    "    raw_orders_df[col] = pd.to_numeric(raw_orders_df[col])\n",
    "\n",
    "# make all the emails lowercase\n",
    "orders_selected_columns[\"Email\"] = orders_selected_columns[\"Email\"].str.lower().str.strip()\n",
    "df = orders_selected_columns[orders_selected_columns[\"Email\"].notna() & (orders_selected_columns[\"Email\"] != \"\")]\n",
    "\n",
    "# After subsetting, the index may be non‐consecutive (e.g., 0, 2, 5, 7). \n",
    "# Resetting the index gives us a clean 0..N−1 index.\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df[\"Created at\"] = pd.to_datetime(df['Created at'].dt.date,format=\"yyyy-mm-dd\")\n",
    "\n",
    "df.to_csv(\"step2.csv\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e22e82",
   "metadata": {},
   "source": [
    "## Step 3 : Feature engineering \n",
    "Here we are going to do feature engineering. At end of the feature engineering each row we will have the following columns to our dataset\n",
    "- Days since first purchase date(this is the different between first purchase date - current date )\n",
    "- Days since second last purchase date (this is the different between second last purchase date - current date)\n",
    "- Purchase interval days between orders. Here will have few columns and they are \n",
    "    - Purchase interval between 1st and 2nd order\n",
    "    - Purchase interval between 2nd and 3nd order\n",
    "    - Purchase interval between 3st and 4nd order\n",
    "    - so on so forth . It will depend on how on how many orders a customer have\n",
    "- Average order value (total / nr of order of each customer)\n",
    "- Target column in days(2nd last purchase - last purchase date)\n",
    "\n",
    "**Note**\n",
    "- Let's think about this brenda.bruinenberg@hotmail.com, this customer. She ordered 7 products on the same date, meaning it's one order with 7 items. So this customer never come back again . So we will remove those customer churn. \n",
    "- We are going to exclude the last order of each customer if he/she order 2 or more orders not 2 or more items  to avoid data leakage for predicting next purchase date, meaning 2 or more order on different dates.\n",
    "\n",
    "belong to feature_engineer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "713fccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdtas\\AppData\\Local\\Temp\\ipykernel_24176\\3520240136.py:95: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  features_df[interval_cols] = features_df[interval_cols].fillna(-1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "STEP2_PATH = Path(\"step2.csv\")        # cleaned orders (line‑item level)\n",
    "# Use the current UTC timestamp at runtime\n",
    "TODAY = pd.Timestamp.now(tz=\"UTC\").normalize()\n",
    "MAX_GAPS   = 10                                 # how many interval columns to keep\n",
    "\n",
    "# 1. Load & tidy the raw orders\n",
    "orders_raw = pd.read_csv(STEP2_PATH)\n",
    "\n",
    "# Drop accidental index column if present\n",
    "orders_raw = orders_raw.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Normalise headers just in case\n",
    "orders_raw.columns = orders_raw.columns.str.strip()\n",
    "\n",
    "# Parse timestamp with timezone\n",
    "orders_raw[\"Created at\"] = pd.to_datetime(orders_raw[\"Created at\"], utc=True)\n",
    "orders_raw.to_csv(\"orders_raw.csv\")\n",
    "\n",
    "# Deduplicate so that one row == one order even if multi‑item\n",
    "# 0   2025-04-10 00:00:00  (T-shirt)\n",
    "# 1   2025-04-10 00:00:00  (Jeans)\n",
    "# 2   2025-04-10 00:00:00  (Cap)\n",
    "# 3   2025-05-15 00:00:00  (Shoes)\n",
    "\n",
    "# 0   2025-04-10 00:00:00\n",
    "# 1   2025-05-15 00:00:00\n",
    "\n",
    "\n",
    "orders = (\n",
    "    orders_raw.sort_values([\"Email\", \"Created at\"])\n",
    "              .drop_duplicates(subset=[\"Email\", \"Created at\"])\n",
    "              .reset_index(drop=True)\n",
    ")\n",
    "orders.to_csv(\"orders.csv\")\n",
    "\n",
    "# 2. Build feature rows\n",
    "rows = []\n",
    "for email, grp in orders.groupby(\"Email\", sort=False):\n",
    "    dates = grp[\"Created at\"].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Keep only customers with ≥2 orders\n",
    "    if len(dates) < 2:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    #     dates = [\n",
    "    #     2025-01-10,   # first order\n",
    "    #     2025-02-15,   # second order\n",
    "    #     2025-04-02,   # third order (second-last)\n",
    "    #     2025-05-20    # fourth order (last)\n",
    "    # ]\n",
    "\n",
    "    target_gap = (dates.iloc[-1] - dates.iloc[-2]).days\n",
    "    \n",
    "    #  drop last order to avoid data leakage\n",
    "    dates_kept = dates.iloc[:-1]\n",
    "\n",
    "\n",
    "    num_orders = len(dates)\n",
    "    total_spend = (\n",
    "        grp.loc[\n",
    "            grp[\"Created at\"].isin(dates),\n",
    "            \"Total\"\n",
    "        ]\n",
    "        .astype(float)\n",
    "        .sum()\n",
    "    )\n",
    "    \n",
    "    #how much money the customer spent prior to their last purchase.\n",
    "    total_spend       = grp.loc[grp[\"Created at\"].isin(dates), \"Total\"].astype(float).sum()\n",
    "    avg_order_value   = total_spend / num_orders\n",
    "    days_since_first  = (TODAY - dates_kept.iloc[0]).days\n",
    "    days_since_2ndlast= (TODAY - dates_kept.iloc[-1]).days\n",
    "    \n",
    "    # Consecutive gaps\n",
    "    gaps = dates_kept.diff().dt.days.dropna().astype(\"Int64\").tolist()\n",
    "    gaps = (gaps + [pd.NA]*MAX_GAPS)[:MAX_GAPS]\n",
    "    \n",
    "    row = {\n",
    "        \"email\": email,\n",
    "        \"num_orders\": num_orders,\n",
    "        \"total_spend\": total_spend,\n",
    "        \"avg_order_value\": avg_order_value,\n",
    "        \"days_since_first_purchase\": days_since_first,\n",
    "        \"days_since_second_last_purchase\": days_since_2ndlast,\n",
    "        \"target_gap_days\": target_gap,\n",
    "    }\n",
    "    row.update({f\"interval_{i+1}_{i+2}\": g for i, g in enumerate(gaps)})\n",
    "    rows.append(row)\n",
    "\n",
    "features_df = pd.DataFrame(rows)\n",
    "\n",
    "# Fill missing intervals with -1\n",
    "interval_cols = [c for c in features_df.columns if c.startswith(\"interval_\")]\n",
    "features_df[interval_cols] = features_df[interval_cols].fillna(-1)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Save & preview\n",
    "# ---------------------------------------------------------------------\n",
    "features_df.to_csv(\"final_features.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81860412",
   "metadata": {},
   "source": [
    "belong to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4b3bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R²: 0.734\n",
      "Test (hold-out) R²: 0.192\n",
      "Saved to: C:\\Users\\Public\\Inholland\\Year 3\\Minor\\3.4\\Moodies Undies\\MoodiesData\\notebooks\\results.csv\n",
      "Current working dir: C:\\Users\\Public\\Inholland\\Year 3\\Minor\\3.4\\Moodies Undies\\MoodiesData\\notebooks\n",
      "CSV exists: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. File paths\n",
    "# ------------------------------------------------------------------\n",
    "FEATURES_PATH = Path(\"final_features.csv\")   # one row per customer\n",
    "ORDERS_PATH   = Path(\"step2.csv\")            # one row per order\n",
    "OUT_PRED_PATH = Path(\"predicted_next_purchase_dates_fixed.csv\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Load feature matrix (has target_gap_days and email)\n",
    "\n",
    "\n",
    "feat = (\n",
    "    pd.read_csv(FEATURES_PATH)\n",
    "      .drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Reconstruct each customer's *last* order date\n",
    "# ------------------------------------------------------------------\n",
    "orders = (\n",
    "    pd.read_csv(ORDERS_PATH)\n",
    "      .drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    ")\n",
    "orders[\"Created at\"] = pd.to_datetime(orders[\"Created at\"], utc=True)\n",
    "\n",
    "# make sure one row per email\n",
    "last_orders = (\n",
    "    orders.groupby(\"Email\")[\"Created at\"]\n",
    "          .last()\n",
    "          .rename(\"last_order_date\")\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Merge last_order_date into the features\n",
    "# ------------------------------------------------------------------\n",
    "df = feat.merge(last_orders, left_on=\"email\", right_on=\"Email\", how=\"left\")\n",
    "\n",
    "# Sanity check\n",
    "if df[\"last_order_date\"].isna().any():\n",
    "    raise ValueError(\"Some emails missing last_order_date after merge.\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5. Build X and y, split train/test\n",
    "# \"target_gap_days\" (the target column)\n",
    "# This column is exactly what we are trying to predict:\n",
    "\n",
    "# The gap between second-last and last purchase (in days).\n",
    "\n",
    "# We remove it from the feature matrix X, because we never include our target column as an input to the model.\n",
    "\n",
    "# It goes into y instead:\n",
    "DROP_COLS = [\n",
    "    \"target_gap_days\",        # target\n",
    "    \"email\", \"Email\",         # id columns\n",
    "    \"last_order_date\"         # timestamp (not numeric)\n",
    "]\n",
    "\n",
    "X = df.drop(columns=DROP_COLS)\n",
    "y = df[\"target_gap_days\"].astype(float)\n",
    "# | Split                | Data            | Purpose                           |\n",
    "# | -------------------- | --------------- | --------------------------------- |\n",
    "# | `X_train`, `y_train` | 80% of the data | Used to train (fit) the model     |\n",
    "# | `X_test`, `y_test`   | 20% of the data | Used to test (evaluate) the model |\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6. Fit Random-Forest\n",
    "# ------------------------------------------------------------------\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    min_samples_leaf=2,#“Each leaf node (final decision point of any tree) must have at least 2 samples.”\n",
    "    #Every time  we run the code, we will get exactly the same trained model (same splits, same trees), which is very useful during development.\n",
    "    random_state=42,\n",
    "    n_jobs=-1 #“Use all available CPU cores.”\n",
    "\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7. Report R²\n",
    "# ------------------------------------------------------------------\n",
    "train_r2 = r2_score(y_train, rf.predict(X_train))\n",
    "test_r2  = r2_score(y_test,  rf.predict(X_test))\n",
    "\n",
    "print(f\"Training R²: {train_r2:.3f}\")\n",
    "print(f\"Test (hold-out) R²: {test_r2:.3f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 8. (Optional) re-fit on *all* data and generate next-purchase dates\n",
    "# ------------------------------------------------------------------\n",
    "rf_all = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ").fit(X, y)\n",
    "\n",
    "pred_gap = rf_all.predict(X)                 # predicted gap in days\n",
    "pred_gap_int = np.round(pred_gap).astype(int)\n",
    "\n",
    "df[\"predicted_gap_days\"] = pred_gap\n",
    "df[\"predicted_next_purchase_date\"] = (\n",
    "    df[\"last_order_date\"] + pd.to_timedelta(pred_gap_int, unit=\"D\")\n",
    ").dt.date\n",
    "\n",
    "result = df[[\"email\", \"predicted_gap_days\", \"predicted_next_purchase_date\"]]\n",
    "OUT_PRED_PATH = Path(\"results.csv\")   # or an absolute path\n",
    "\n",
    "result.to_csv(OUT_PRED_PATH, index=False)   # ← suppress the index column\n",
    "print(f\"Saved to: {OUT_PRED_PATH.resolve()}\")   # confirms the exact location\n",
    "import os, pathlib\n",
    "print(\"Current working dir:\", pathlib.Path().resolve())\n",
    "print(\"CSV exists:\", OUT_PRED_PATH.exists())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
